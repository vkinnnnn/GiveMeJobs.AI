# Logstash Configuration for GiveMeJobs Platform
input {
  # Beats input for log shipping
  beats {
    port => 5044
  }
  
  # Syslog input
  syslog {
    port => 5514
  }
  
  # HTTP input for application logs
  http {
    port => 8080
    codec => json
  }
  
  # Kubernetes logs via Filebeat
  beats {
    port => 5045
    type => "kubernetes"
  }
}

filter {
  # Parse Kubernetes logs
  if [kubernetes] {
    # Extract pod information
    mutate {
      add_field => {
        "pod_name" => "%{[kubernetes][pod][name]}"
        "namespace" => "%{[kubernetes][namespace]}"
        "container_name" => "%{[kubernetes][container][name]}"
      }
    }
    
    # Parse JSON logs from applications
    if [container_name] in ["backend", "python-services", "frontend"] {
      json {
        source => "message"
        target => "app_log"
      }
      
      # Extract structured log fields
      if [app_log] {
        mutate {
          add_field => {
            "log_level" => "%{[app_log][level]}"
            "correlation_id" => "%{[app_log][correlation_id]}"
            "user_id" => "%{[app_log][user_id]}"
            "request_id" => "%{[app_log][request_id]}"
          }
        }
        
        # Parse timestamp
        date {
          match => [ "[app_log][timestamp]", "ISO8601" ]
        }
      }
    }
  }
  
  # Parse Nginx access logs
  if [container_name] == "nginx" {
    grok {
      match => { 
        "message" => "%{NGINXACCESS}"
      }
    }
    
    # Convert response time to number
    mutate {
      convert => { "response_time" => "float" }
      convert => { "response" => "integer" }
    }
    
    # Add response time categories
    if [response_time] {
      if [response_time] < 0.1 {
        mutate { add_field => { "response_time_category" => "fast" } }
      } else if [response_time] < 1.0 {
        mutate { add_field => { "response_time_category" => "normal" } }
      } else if [response_time] < 5.0 {
        mutate { add_field => { "response_time_category" => "slow" } }
      } else {
        mutate { add_field => { "response_time_category" => "very_slow" } }
      }
    }
  }
  
  # Parse PostgreSQL logs
  if [container_name] == "postgres" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:pid}\] %{WORD:log_level}:  %{GREEDYDATA:postgres_message}"
      }
    }
    
    # Extract slow query information
    if [postgres_message] =~ /duration: (\d+\.\d+) ms/ {
      grok {
        match => { 
          "postgres_message" => "duration: %{NUMBER:query_duration:float} ms  statement: %{GREEDYDATA:sql_statement}"
        }
      }
      
      # Mark slow queries
      if [query_duration] and [query_duration] > 1000 {
        mutate { add_field => { "slow_query" => "true" } }
      }
    }
  }
  
  # Parse Python application logs (structured JSON)
  if [container_name] == "python-services" {
    json {
      source => "message"
      target => "python_log"
    }
    
    if [python_log] {
      # Extract Python-specific fields
      mutate {
        add_field => {
          "service_name" => "%{[python_log][service]}"
          "function_name" => "%{[python_log][function]}"
          "execution_time" => "%{[python_log][execution_time]}"
          "ai_model" => "%{[python_log][ai_model]}"
          "tokens_used" => "%{[python_log][tokens_used]}"
        }
      }
      
      # Convert numeric fields
      mutate {
        convert => { "execution_time" => "float" }
        convert => { "tokens_used" => "integer" }
      }
    }
  }
  
  # Add common fields
  mutate {
    add_field => {
      "environment" => "production"
      "platform" => "givemejobs"
    }
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => ["agent", "ecs", "host", "input"]
  }
  
  # GeoIP enrichment for client IPs
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "givemejobs-logs-%{+YYYY.MM.dd}"
    
    # Use document type based on log source
    template_name => "givemejobs-logs"
    template => "/usr/share/logstash/templates/givemejobs-template.json"
    template_overwrite => true
  }
  
  # Output critical errors to separate index
  if [log_level] == "ERROR" or [log_level] == "CRITICAL" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "givemejobs-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # Output security events to separate index
  if [container_name] == "nginx" and [response] >= 400 {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "givemejobs-security-%{+YYYY.MM.dd}"
    }
  }
  
  # Debug output (remove in production)
  # stdout { codec => rubydebug }
}